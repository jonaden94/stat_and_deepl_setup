{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b1cd984",
   "metadata": {},
   "source": [
    "## Simple tutorial on exploratory model building on a subset of the Airbnb dataset.\n",
    "\n",
    "To run this notebook, you need to download the data from [here](https://insideairbnb.com/get-the-data) and put it in the same directory as this notebook. The functionality of this notebook was tested with the data from Vienna.\n",
    "\n",
    "\n",
    "In this introductory notebook, we will take a closer look at the Airbnb data. This is by no means a \"perfect\" analysis, but serves only as an introduction to how one might approach a problem such as the one at hand. So the analyses performed are only meant as a kind of inspiration for your own models.\n",
    "\n",
    "The variable selection is arbitrary and not every potentially valuable variable is used. We will for instance only use some of the structural variables and the images, but no text data.\n",
    "\n",
    "In this simple tutorial you will learn:\n",
    "\n",
    "* How to read in and read out the images for modeling.\n",
    "* How to deal with missing data\n",
    "    * Simple imputation transformations to fill in missing values, such as using the median\n",
    "* How to preprocess structural variables\n",
    "* Creating base models for the structural variables for data exploration.\n",
    "    * Simple linear regression\n",
    "    * Decision Tree Regression\n",
    "    * Random Forest Regression\n",
    "* How to create a Multi Layer Perceptron (MLP) with Pytorch.\n",
    "* How to build a basic Convolutional Neural Network (CNN) with Pytorch.\n",
    "* Predicting the RMSE using your CNN architecture.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde4198d",
   "metadata": {},
   "source": [
    "## Index\n",
    "#### 1. Load the data and scrape the images\n",
    "\n",
    "#### 2. Data Exploration\n",
    "\n",
    "#### 3. Model building using only the structural variables\n",
    "\n",
    "#### 4. Model building using the images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3ed73b",
   "metadata": {},
   "source": [
    "# 1. Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7ca1d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import matplotlib.image as mpimg\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463c5a0b",
   "metadata": {},
   "source": [
    "I chose only to use the first 500 observations of the available data to have a faster computation time.\n",
    "In this tutorial we will not use the 4 datasets loaded below but will solely focus on the listings.csv.gz file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb3b7bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "listings = pd.read_csv(\"listings.csv\")[0:1500]\n",
    "reviews = pd.read_csv(\"reviews.csv\")[0:1500]\n",
    "reviews_meta = pd.read_csv(\"reviews.csv.gz\")[0:1500]\n",
    "calendar = pd.read_csv(\"calendar.csv.gz\")[0:1500]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f813d3",
   "metadata": {},
   "source": [
    "Load the main data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87f18055",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"listings.csv.gz\")[0:1500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9a9c68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all rows where variables price, host_is_superhost or pictures are missing\n",
    "train_df = train_df.dropna(subset=[\"picture_url\", \"host_picture_url\", \"price\", \"host_is_superhost\"])\n",
    "train_df = train_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4c9932",
   "metadata": {},
   "source": [
    "Unfortunately, the variable we are interested in, the price, is given in a string format in the form of  \"10.00$\".\n",
    "\n",
    "For easier computation we transform these strings into floats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10dfcc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform price\n",
    "train_df[\"price\"] = (\n",
    "    train_df[\"price\"].str.replace(\"$\", \"\").str.replace(\",\", \"\").astype(float)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed3da68",
   "metadata": {},
   "source": [
    "Additionaly let's compute the log price, as predicting the log price gives us some advantages we will see later in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31dd0dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"log_price\"] = np.log(train_df[\"price\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae6435c",
   "metadata": {},
   "source": [
    "As you can see below, we have an enormous amount of available data (75 columns + additional files). However, some of these variables contain essentially the same or extremely correlated information. For example, I would guess that the sentiment of a written review is highly correlated with the given *star* rating."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b87642",
   "metadata": {},
   "source": [
    "Let's have a quick look at the data. We have 75 columns with a lot of variables that probably do not bare any explanation in them at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8d7a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99504b1d",
   "metadata": {},
   "source": [
    "### \"Scrape\" the images\n",
    "\n",
    "First, we try to scrape the available images: \n",
    "\n",
    "This loop, as terrible as it is written, loops through the given flat-picture and host-picture URL's and appends the scraped pictures to two lists. We resize the images to a size of 224 by 224 pixels and normalize them by dividing through 255 (the maximum pixel value). All images that cannot be scraped or are not with rgb are dropped from the dataframe. This is obviously optional and can be handled differently by your approach.\n",
    "\n",
    "The used picture size of 224x224 is also done randomly. Have in mind, however, that larger pixel values (while maybe giving you better prediction results) will give you longer computation times. However, using e.g. 8x8 images as is sometimes done with the MNIST data sets will give you unrecognizable flat images.\n",
    "\n",
    "Depending on your number of observations this can take a while. Therefore it is sensible for you (if you haven't already) to start your analysis by scrapping all images and (obviously) saving them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304e48ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import requests\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "pictures_flat = []\n",
    "pictures_host = []\n",
    "drop_indices = []\n",
    "\n",
    "for i in tqdm(range(len(train_df))):\n",
    "    try:\n",
    "        # Get image URLs\n",
    "        url_flat = train_df[\"picture_url\"].iloc[i]\n",
    "        url_host = train_df[\"host_picture_url\"].iloc[i]\n",
    "\n",
    "        # Try downloading the images with timeout and error handling\n",
    "        response_flat = requests.get(url_flat, timeout=5)\n",
    "        response_host = requests.get(url_host, timeout=5)\n",
    "\n",
    "        response_flat.raise_for_status()\n",
    "        response_host.raise_for_status()\n",
    "\n",
    "        # Open, resize, normalize\n",
    "        img_flat = Image.open(BytesIO(response_flat.content)).convert(\"RGB\").resize((64, 64))\n",
    "        img_host = Image.open(BytesIO(response_host.content)).convert(\"RGB\").resize((64, 64))\n",
    "\n",
    "        img_flat = np.array(img_flat) / 255.0\n",
    "        img_host = np.array(img_host) / 255.0\n",
    "\n",
    "        # Final shape check\n",
    "        if img_flat.shape != (64, 64, 3) or img_host.shape != (64, 64, 3):\n",
    "            raise ValueError(\"Unexpected image shape\")\n",
    "\n",
    "        # Append processed images\n",
    "        pictures_flat.append(img_flat)\n",
    "        pictures_host.append(img_host)\n",
    "\n",
    "    except Exception as e:\n",
    "        # Log error and mark index for dropping\n",
    "        print(f\"Skipping index {i} due to error: {e}\")\n",
    "        drop_indices.append(i)\n",
    "\n",
    "# Drop bad rows from DataFrame\n",
    "train_df = train_df.drop(index=drop_indices).reset_index(drop=True)\n",
    "train_df.to_pkl(\"train_df.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7105781",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_pickle(\"train_df.pkl\")\n",
    "pictures_host = train_df[\"pic_host\"]\n",
    "pictures_flat = train_df[\"pic_flat\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89e1b79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_flat = np.asarray(pictures_flat)\n",
    "train_images_host = np.asarray(pictures_host)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959ca479",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.median(train_df[\"price\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518ad6a2",
   "metadata": {},
   "source": [
    "# 2. Data Exploration\n",
    "\n",
    "First we take a closer look at the price distribution of the given data.\n",
    "We can clearly see that we have a wide range of prices and very few observations at the very top of the price range.\n",
    "The average airbnb apartment is rented for roughly 100$ per night. The median lies is a little lower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987de97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={\"figure.figsize\": (15, 5)})\n",
    "fig = plt.figure()\n",
    "sns.histplot(data=train_df, x=\"price\", bins=50)\n",
    "plt.axvline(train_df[\"price\"].mean(), c=\"red\", ls=\"-\", lw=3, label=\"Mean Price\")\n",
    "plt.axvline(train_df[\"price\"].median(), c=\"blue\", ls=\"-\", lw=3, label=\"Median Price\")\n",
    "plt.title(\"Distribution of Airbnb Prices\", fontsize=20, fontweight=\"bold\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0baee5c4",
   "metadata": {},
   "source": [
    "Sometimes it might be advisable to take a closer look at the log-distribution of the prices, which, often depending on the chosen sample size, looks pretty much like a normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0b9868",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={\"figure.figsize\": (15, 5)})\n",
    "fig = plt.figure()\n",
    "sns.histplot(data=train_df, x=\"log_price\", bins=50)\n",
    "plt.axvline(train_df[\"log_price\"].mean(), c=\"red\", ls=\"-\", lw=3, label=\"Mean Log Price\")\n",
    "plt.axvline(\n",
    "    train_df[\"log_price\"].median(),\n",
    "    c=\"blue\",\n",
    "    ls=\"-\",\n",
    "    lw=3,\n",
    "    label=\"Median Log Price\",\n",
    ")\n",
    "plt.title(\"Distribution of Airbnb Log Prices\", fontsize=20, fontweight=\"bold\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd52c75",
   "metadata": {},
   "source": [
    "Additionaly we take a closer look at some of the variables which are chosen at random, although I do expect that the chosen 3 variables, namely the room type, whether the host is a so-called \"superhost\" and the number of people the apartment accomodates has a significant impact on the rental price. However, this is by no means validated and thus you should not be too strongly influenced by my choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f2e4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_variables = [\n",
    "    \"room_type\",\n",
    "    \"host_is_superhost\",\n",
    "    \"accommodates\",\n",
    "]\n",
    "\n",
    "# for each of the above listed feature variables\n",
    "# show a boxplot and distribution plot against the log price\n",
    "for variable in feature_variables:\n",
    "    fig, ax = plt.subplots(1, 2)\n",
    "    sns.boxplot(data=train_df, x=variable, y=\"log_price\", ax=ax[0])\n",
    "    sns.histplot(train_df, x=\"log_price\", hue=variable, kde=True, ax=ax[1])\n",
    "    plt.suptitle(variable, fontsize=20, fontweight=\"bold\")\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d895bb29",
   "metadata": {},
   "source": [
    "Now, we take a look at the host images and the flat images. For this, we write a simple function that plots the stored images and labels them with the given apartment rental price. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ebe63eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images_and_prices(\n",
    "    df, desired_price=None, num_images=5, random_state=202, pics=\"pic_host\"\n",
    "):\n",
    "    \"\"\"plots images and respective prices\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame eveyrthing is stored in. Prices and images as arrays\n",
    "        desired_price (float, optional): If you want to only view images with a specified rental price. Defaults to None.\n",
    "        num_images (int, optional): Number of images to be plotted. Defaults to 5.\n",
    "        random_state (int, optional): . Defaults to 101.\n",
    "        pics (str, optional): column Name in df where images are stored. Defaults to \"pic_host\".\n",
    "    \"\"\"\n",
    "\n",
    "    num_images = num_images\n",
    "\n",
    "    # set the rample state for the sampling for reproducibility\n",
    "    random_state = random_state\n",
    "\n",
    "    # only select entries with given price when specified\n",
    "    if desired_price != None:\n",
    "        random_sample = (\n",
    "            df[df[\"price\"] == desired_price]\n",
    "            .sample(num_images, random_state=random_state)\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "    else:\n",
    "        random_sample = df.sample(num_images, random_state=random_state)\n",
    "\n",
    "    for i in range(num_images):\n",
    "\n",
    "        price = random_sample.iloc[i][\"price\"]\n",
    "        plt.subplot(1, num_images, i + 1)\n",
    "\n",
    "        title = price\n",
    "        plt.title(title)\n",
    "\n",
    "        # turn off gridlines\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        plt.imshow(random_sample.iloc[i][pics])\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0e2e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the hosts\n",
    "plot_images_and_prices(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f2fe61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the flats\n",
    "plot_images_and_prices(train_df, pics=\"pic_flat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520f9ec4",
   "metadata": {},
   "source": [
    "# 3. Model building"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ecd932",
   "metadata": {},
   "source": [
    "To create a model, simpler models are first examined. This can be used, for example, to select the appropriate variables, since simple models often offer good interpretability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1dbac9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import matplotlib.patches as mpatches\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.impute import SimpleImputer\n",
    "from math import e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c988b124",
   "metadata": {},
   "source": [
    "First, we select only a few of the variables for easier computation. An adewuate variable selection will be of crucial importance for your model, however, for this simple introduction I just picked what I thought might be reasonable. Additionaly, I chose variables that might be correlated with one another as bedrooms and beds, but did not adjust for that in my modeling as you might see later.\n",
    "\n",
    "We choose the log-price as Y, as we will start with a simple linear regression where the normality assumption is crucial.\n",
    "\n",
    "As the chosen feature variables we selected:\n",
    "\n",
    "* \"host_is_superhost\"\n",
    "* \"latitude\"\n",
    "* \"longitude\"\n",
    "* \"room_type\"\n",
    "* \"accommodates\"\n",
    "* \"bedrooms\"\n",
    "* \"number_of_reviews\"\n",
    "* \"review_scores_value\"\n",
    "\n",
    "\n",
    "Most of these are self explanatory, but you can look up their exact meaning at: https://docs.google.com/spreadsheets/d/1iWCNJcSutYqpULSQHlNyGInUvHg2BoUGoNRIGa6Szc4/edit#gid=982310896"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "12f9d8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = train_df[\"log_price\"]\n",
    "\n",
    "train_df = train_df[\n",
    "    [\n",
    "        \"host_is_superhost\",\n",
    "        \"latitude\",\n",
    "        \"longitude\",\n",
    "        \"room_type\",\n",
    "        \"accommodates\",\n",
    "        \"bedrooms\",\n",
    "        \"minimum_nights\",\n",
    "        \"number_of_reviews\",\n",
    "        \"review_scores_value\",\n",
    "        \"host_identity_verified\",\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc82261",
   "metadata": {},
   "source": [
    "There are several ways to deal with missing data. The obvious one is to simply drop all rows where you encounter any missing (or unreasonable, e.g. 10 billion beds) values. Sometimes this might even be advisable. However, I chose to use a fairly straight forward method by simply replacing the missing values with the respective median values of the features. I chose the median, as the mean of e.g. number of bedrooms is not really meaningful, with e.g. 1.19.\n",
    "\n",
    "Additionally we dummy decode the column \"room_type\", hus creating additional columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5350114",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.isna().sum() # When considering only the first 150 images, there are not so many missing values. Data processing will get important when using the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "54748873",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "\n",
    "\n",
    "imputer = imputer.fit(\n",
    "    train_df[\n",
    "        [\n",
    "            \"accommodates\",\n",
    "            \"bedrooms\",\n",
    "            \"number_of_reviews\",\n",
    "            \"review_scores_value\",\n",
    "            \"minimum_nights\",\n",
    "        ]\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_df[\n",
    "    [\n",
    "        \"accommodates\",\n",
    "        \"bedrooms\",\n",
    "        \"number_of_reviews\",\n",
    "        \"review_scores_value\",\n",
    "        \"minimum_nights\",\n",
    "    ]\n",
    "] = imputer.transform(\n",
    "    train_df[\n",
    "        [\n",
    "            \"accommodates\",\n",
    "            \"bedrooms\",\n",
    "            \"number_of_reviews\",\n",
    "            \"review_scores_value\",\n",
    "            \"minimum_nights\",\n",
    "        ]\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a25d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "rooms = pd.get_dummies(train_df[\"room_type\"], prefix=\"room\")\n",
    "train_df = train_df.drop(\"room_type\", axis=1)\n",
    "train_df[\"host_is_superhost\"] = train_df[\"host_is_superhost\"].map(dict(t=1, f=0))\n",
    "train_df[\"host_identity_verified\"] = train_df[\"host_identity_verified\"].map(\n",
    "    dict(t=1, f=0)\n",
    ")\n",
    "train_df = pd.concat([train_df, rooms], axis=1)\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168ee214",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f19aa4",
   "metadata": {},
   "source": [
    "For a visual interpretation of our following model results we define a simple plotting function that plots the actual prices vs our predicted prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7c98b9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see what our predictions look like vs the actual\n",
    "def ActualvPredictionsGraph(test_y, pred_y, title, prob=False):\n",
    "    if max(test_y) >= max(pred_y):\n",
    "        my_range = int(max(test_y))\n",
    "    else:\n",
    "        my_range = int(max(pred_y))\n",
    "    plt.figure(figsize=(12, 3))\n",
    "    plt.scatter(range(len(test_y)), test_y, color=\"blue\")\n",
    "    plt.scatter(range(len(pred_y)), pred_y, color=\"red\")\n",
    "    plt.xlabel(\"Index \")\n",
    "    plt.ylabel(\"Log_price\")\n",
    "    plt.title(title, fontdict={\"fontsize\": 15})\n",
    "    plt.legend(\n",
    "        handles=[\n",
    "            mpatches.Patch(color=\"red\", label=\"prediction\"),\n",
    "            mpatches.Patch(color=\"blue\", label=\"actual\"),\n",
    "        ]\n",
    "    )\n",
    "    plt.show()\n",
    "\n",
    "    if prob:\n",
    "        # plot actual v predicted in histogram form\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        sns.histplot(predict_test, color=\"r\", alpha=0.3, stat=\"probability\", kde=True)\n",
    "        sns.histplot(test_y, color=\"b\", alpha=0.3, stat=\"probability\", kde=True)\n",
    "        plt.legend(labels=[\"prediction\", \"actual\"])\n",
    "        plt.title(\"Actual v Predict Distribution\" + str(title))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40521d24",
   "metadata": {},
   "source": [
    "Before we start modelling, we need to agree on an evaluation method. Obviously for a simple linear regression we could use the R^2, adjusted R2 or AIC metric, but as later we are introducing neural networks I want to use a simple and straight forward method of evluating the model predictions. For that purpose we will use the Root Mean Square Error (RMSE) metric.\n",
    "\n",
    "\\begin{equation}\n",
    "RMSE = \\sqrt {\\sum_{i=1}^{n} \\frac{\\hat{y_i} - y_i}{n}},\n",
    "\\end{equation}\n",
    "\n",
    "where $\\hat{y}$ is the predicted price. Be aware that the imported function is only the Mean Squared Error (MSE) which is why we take the squareroot by using **0.5\n",
    "\n",
    "\n",
    "For that purpose we will split our data set into a training data set and a testing data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9cf047ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, test_x, train_y, test_y = train_test_split(\n",
    "    train_df, Y, test_size=0.25, random_state=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86619003",
   "metadata": {},
   "source": [
    "First, lets try a \"stupid model\" that only predicts the mean of the training data, in our case the mean log price of the training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fb88d70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_basic_model_prediction(y_train, y_test):\n",
    "    a = np.empty(len(test_y))\n",
    "    a.fill(np.mean(train_y))\n",
    "    return a\n",
    "\n",
    "\n",
    "pred_y = most_basic_model_prediction(train_y, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f099dba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE on test data:  0.5278642627476925\n"
     ]
    }
   ],
   "source": [
    "print(\"RMSE on test data: \", mean_squared_error(test_y, pred_y) ** (0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fe1d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "ActualvPredictionsGraph(test_y, pred_y, \"Actual v. Mean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0a5e48",
   "metadata": {},
   "source": [
    "## Linear Regression Model\n",
    "\n",
    "You should all be familiar with the simple linear regression model. \n",
    "\\begin{equation}\n",
    "\\mathbf{Y} = \\mathbf{X}\\mathbf{\\beta} + \\mathbf{\\epsilon}\n",
    "\\end{equation}\n",
    "However, you might not have fitted a regression using python. \n",
    "\n",
    "To use a linear regression in python is similarly easy as it is using it in R. Unfortunately, sklearn does not offer a summary as nice as the R-version does. If you are interested in a similar regression output you could use e.g. statsmodels or create the output yourself by e.g. using sklearns metrics as the introduced MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fea37bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_LR = LinearRegression()\n",
    "\n",
    "# fit the model with the training data\n",
    "model_LR.fit(train_x, train_y)\n",
    "\n",
    "predict_train = model_LR.predict(train_x)\n",
    "predict_test = model_LR.predict(test_x)\n",
    "# Root Mean Squared Error on train and test date\n",
    "\n",
    "print(\"RMSE on train data: \", mean_squared_error(train_y, predict_train) ** (0.5))\n",
    "print(\"RMSE on test data: \", mean_squared_error(test_y, predict_test) ** (0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08541313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot it\n",
    "ActualvPredictionsGraph(test_y, predict_test, \"Actual v. Predicted LM\", prob=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e9260f",
   "metadata": {},
   "source": [
    "Decision trees are algorithms that can be used in both, regression and classification. They are basically using a set of binary rules and are thus very easy to interpret. A decision tree, just as a real tree has subsequently branches, nodes and leaves. See [here](https://www.datacamp.com/tutorial/decision-tree-classification-python) to get the basic idea of a decision tree if you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a02d976a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE on train data:  0.41953133106249946\n",
      "RMSE on test data:  0.46540733261356293\n"
     ]
    }
   ],
   "source": [
    "tree_reg = DecisionTreeRegressor(max_depth=3, min_samples_split=5)\n",
    "\n",
    "# train the model\n",
    "tree_reg.fit(train_x, train_y)\n",
    "\n",
    "# predict the response for the test data\n",
    "predict_train = tree_reg.predict(train_x)\n",
    "predict_test = tree_reg.predict(test_x)\n",
    "\n",
    "# Root Mean Squared Error on train and test date\n",
    "print(\"RMSE on train data: \", mean_squared_error(train_y, predict_train) ** (0.5))\n",
    "print(\"RMSE on test data: \", mean_squared_error(test_y, predict_test) ** (0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ace21a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ActualvPredictionsGraph(test_y, predict_test, \"Actual v. Predicted DT\", prob=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b4af04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the decision tree\n",
    "fig = plt.figure(figsize=(15, 5))\n",
    "plot = tree.plot_tree(\n",
    "    tree_reg, feature_names=train_x.columns.values.tolist(), filled=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eadbf04",
   "metadata": {},
   "source": [
    "The overall performance of the decision trees is fairly similar to the performance of a simple linear regression. However, wee see an accumulation of predictions around certain prices due to the inherent model structure of binary decisions.\n",
    "\n",
    "A similar and very common approach is a random forest regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "16d53509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE on train data:  0.17320630562633219\n",
      "RMSE on test data:  0.39184350231160225\n"
     ]
    }
   ],
   "source": [
    "model_RFR = RandomForestRegressor(max_depth=10)\n",
    "\n",
    "# fit the model with the training data\n",
    "model_RFR.fit(train_x, train_y)\n",
    "\n",
    "# predict the target on train and test data\n",
    "predict_train = model_RFR.predict(train_x)\n",
    "predict_test = model_RFR.predict(test_x)\n",
    "\n",
    "# Root Mean Squared Error on train and test date\n",
    "print(\"RMSE on train data: \", mean_squared_error(train_y, predict_train) ** (0.5))\n",
    "print(\"RMSE on test data: \", mean_squared_error(test_y, predict_test) ** (0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f49fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ActualvPredictionsGraph(test_y, predict_test, \"Actual v. Predicted RF\", prob=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae3896e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 7))\n",
    "feat_importances = pd.Series(model_RFR.feature_importances_, index=train_x.columns)\n",
    "feat_importances.nlargest(train_df.shape[1]).plot(kind=\"barh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b69008e",
   "metadata": {},
   "source": [
    "This plot gives the \"importance\" of our input variables/features in the model. This could e.g. give us a reason to drop some of our used variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c65c52",
   "metadata": {},
   "source": [
    "Now we want to build our first Multi Layer Perceptron. In this Notebook I will be using firstly the very inflexible MLPRegressor from scikit-learn and secondly a simple Pytorch Implementation. You are, however, by no means in any way restricted and can use any other library you find suitable.\n",
    "\n",
    "Commonly the weights in neural networks are initialized to random small numbers (most often with a random draw from a truncated normal distribution with mean=0 and variance=1) and updated with a given (and often changing/flexible) small learning rate.\n",
    "Given this use of small weights the scale of inputs (and outputs) is an important factor. Unscaled input variables can result in a slow or unstable learning process which in turn can lead to a bad performance. Unscaled output variables (in our case the price) can lead to exploding gradients.\n",
    "\n",
    "As we have already scaled our prices with a log-transformation we will apply a MinMax Scaling approach to our input variables.\n",
    "\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "x_{scaled} = \\frac{x - min(x)}{max(x) - min(x)}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "As you can see in the formula this has no effect on the dummy variables. However, our implementation has an effect on the \"categorical\" variables as e.g. \"accommodates\" or \"beds\". It might therefore maybe be more approriate to use e.g. One-Hot-Encoding for the categorical variables. I have not implemented this in the present notebook as this is purely introductory, so you should definetly have in mind that the variables selection, feature extraction and preprocessing is a very important part of your model building and should not simply be copied from this notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3f229d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try with minmax scaled data -> also usefull for MLP\n",
    "mn = MinMaxScaler()\n",
    "x_train_scaled = pd.DataFrame(mn.fit_transform(train_x), columns=train_x.columns)\n",
    "x_test_scaled = pd.DataFrame(mn.fit_transform(test_x), columns=test_x.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a25b5ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "MLPreg = MLPRegressor(\n",
    "    hidden_layer_sizes=(10,10,10),\n",
    "    activation=\"relu\",\n",
    "    random_state=1,\n",
    "    max_iter=10000,\n",
    ").fit(x_train_scaled, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "419452c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE on train data:  0.4325506982942492\n",
      "RMSE on test data:  0.44385867380905664\n"
     ]
    }
   ],
   "source": [
    "predict_train = MLPreg.predict(x_train_scaled)\n",
    "predict_test = MLPreg.predict(x_test_scaled)\n",
    "# Root Mean Squared Error on train and test data\n",
    "print(\"RMSE on train data: \", mean_squared_error(train_y, predict_train) ** (0.5))\n",
    "print(\"RMSE on test data: \", mean_squared_error(test_y, predict_test) ** (0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72b0de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ActualvPredictionsGraph(test_y, predict_test, \"Actual v. Predicted MLP\", prob=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b211ba7b",
   "metadata": {},
   "source": [
    "Now let's implement a simple MLP with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "162759be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "abb0c2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First define the validation split, number of epochs, and the batch size\n",
    "split = 0.2\n",
    "epochs = 2000\n",
    "batch_size = 64\n",
    "\n",
    "# Convert pandas DataFrames/Series to numpy arrays first\n",
    "x_train_tensor = torch.tensor(x_train_scaled.to_numpy(), dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(train_y.to_numpy(), dtype=torch.float32).view(-1, 1)\n",
    "x_test_tensor = torch.tensor(x_test_scaled.to_numpy(), dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(test_y.to_numpy(), dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "# Create dataset and dataloaders\n",
    "full_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "val_size = int(len(full_dataset) * split)\n",
    "train_size = len(full_dataset) - val_size\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2dba2991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleMLP(\n",
      "  (fc): Linear(in_features=12, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Define a super simple model with just 1 layer\n",
    "\n",
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(SimpleMLP, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "model = SimpleMLP(x_train_scaled.shape[1])\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "59760097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b43cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    epoch_train_loss = 0\n",
    "    for xb, yb in train_loader:\n",
    "        pred = model(xb)\n",
    "        loss = criterion(pred, yb)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_train_loss += loss.item() * xb.size(0)\n",
    "\n",
    "    train_losses.append(epoch_train_loss / train_size)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = sum(criterion(model(xb), yb).item() * xb.size(0) for xb, yb in val_loader)\n",
    "        val_losses.append(val_loss / val_size)\n",
    "\n",
    "    if (epoch + 1) % 20 == 0 or epoch == 0:\n",
    "        print(f\"Epoch {epoch+1}: Train Loss = {train_losses[-1]:.4f}, Val Loss = {val_losses[-1]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1909707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4.2: Plot the learning curve\n",
    "plt.plot(train_losses, label=\"train\")\n",
    "plt.plot(val_losses, label=\"val\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7382387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test data\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predict_train = model(x_train_tensor).numpy()\n",
    "    predict_test = model(x_test_tensor).numpy()\n",
    "\n",
    "print(\"RMSE on train data: \", mean_squared_error(train_y, predict_train) ** 0.5)\n",
    "print(\"RMSE on test data: \", mean_squared_error(test_y, predict_test) ** 0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b466fe3",
   "metadata": {},
   "source": [
    "# Activation Functions\n",
    "\n",
    "For this implementation, we will use the standard Rectified Linear Uni activation function. However there are obviously multiple other alternatives which could be used.\n",
    "\n",
    "### Rectified Linear Activation Function:\n",
    "\n",
    "\\begin{equation}\n",
    "f (x) = \\left\\{\n",
    "\\begin{array}{ll}\n",
    "x & x > 0 \\\\\n",
    "0 & \\, \\textrm{else} \\\\\n",
    "\\end{array}\n",
    "\\right.\n",
    "\\end{equation}\n",
    "\n",
    "The ReLU activation function is pretty simple. It is 0, once x is smaller or equal to zero and x, whenever x is bigger than zero. We could write this function in a loop simply as:\n",
    "```\n",
    "outputs = []\n",
    "for value in inputs:\n",
    "    if value > 0:\n",
    "        outputs.append(x)\n",
    "    elif value <= 0:\n",
    "        outputs.append(0)    \n",
    "```\n",
    "\n",
    "However, this can easily be done in one line, by simply taking the maximum between the value itself and 0:\n",
    "\n",
    "```\n",
    "np.maximum(0, value)\n",
    "```\n",
    "\n",
    "The derivative of the ReLU function is equally as easy and simply not defined for x = 0:\n",
    "\n",
    "\\begin{equation}\n",
    "f (x) = \\left\\{\n",
    "\\begin{array}{ll}\n",
    "1 & x > 0 \\\\\n",
    "0 & x < 0 \\\\\n",
    "\\end{array}\n",
    "\\right.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "233543ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(inputs):\n",
    "    return np.maximum(0, inputs)\n",
    "\n",
    "def linear(inputs):\n",
    "    return inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98742597",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-10, 10, 500)\n",
    "plt.plot(x, ReLU(x))\n",
    "plt.title(\"Activation Function :ReLu\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d30f36",
   "metadata": {},
   "source": [
    "More Complex MLP with PyTorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cb3d7489",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "76f80098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "split = 0.2\n",
    "epochs = 2000\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "early_stopping_patience = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cd04401c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert pandas DataFrames/Series to PyTorch tensors\n",
    "x_train_tensor = torch.tensor(x_train_scaled.to_numpy(), dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(train_y.to_numpy(), dtype=torch.float32).view(-1, 1)\n",
    "x_test_tensor = torch.tensor(x_test_scaled.to_numpy(), dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(test_y.to_numpy(), dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "# Dataset and DataLoader\n",
    "full_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "val_size = int(len(full_dataset) * split)\n",
    "train_size = len(full_dataset) - val_size\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee7b1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complex MLP with ReLU activations and optional dropout\n",
    "class ComplexMLP(nn.Module):\n",
    "    def __init__(self, input_dim, use_dropout=False):\n",
    "        super().__init__()\n",
    "        layers = [\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU()\n",
    "        ]\n",
    "        \n",
    "        if use_dropout:\n",
    "            layers.append(nn.Dropout(0.5))\n",
    "        \n",
    "        layers += [\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        ]\n",
    "\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Instantiate the model\n",
    "model = ComplexMLP(x_train_tensor.shape[1], use_dropout=False)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "033b6eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training setup\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb206a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training with manual early stopping\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    epoch_train_loss = 0\n",
    "    for xb, yb in train_loader:\n",
    "        pred = model(xb)\n",
    "        loss = criterion(pred, yb)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_train_loss += loss.item() * xb.size(0)\n",
    "\n",
    "    epoch_train_loss /= train_size\n",
    "    train_losses.append(epoch_train_loss)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = sum(criterion(model(xb), yb).item() * xb.size(0) for xb, yb in val_loader)\n",
    "        val_loss /= val_size\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: Train Loss = {epoch_train_loss:.4f}, Val Loss = {val_loss:.4f}\")\n",
    "\n",
    "    # Early stopping logic\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        best_model_state = model.state_dict()\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= early_stopping_patience:\n",
    "            print(f\"Stopping early at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "# Load best model\n",
    "model.load_state_dict(best_model_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67feb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the learning curve\n",
    "plt.plot(train_losses, label=\"train\")\n",
    "plt.plot(val_losses, label=\"val\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eca8a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on train and test data\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predict_train = model(x_train_tensor).numpy()\n",
    "    predict_test = model(x_test_tensor).numpy()\n",
    "\n",
    "print(\"RMSE on train data: \", mean_squared_error(train_y, predict_train) ** 0.5)\n",
    "print(\"RMSE on test data: \", mean_squared_error(test_y, predict_test) ** 0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6997abb5",
   "metadata": {},
   "source": [
    "Using Dropout to Mitigate Overfitting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b654b0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reinitialize model with dropout and retrain\n",
    "model = ComplexMLP(x_train_tensor.shape[1], use_dropout=True)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    epoch_train_loss = 0\n",
    "    for xb, yb in train_loader:\n",
    "        pred = model(xb)\n",
    "        loss = criterion(pred, yb)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_train_loss += loss.item() * xb.size(0)\n",
    "\n",
    "    epoch_train_loss /= train_size\n",
    "    train_losses.append(epoch_train_loss)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = sum(criterion(model(xb), yb).item() * xb.size(0) for xb, yb in val_loader)\n",
    "        val_loss /= val_size\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: Train Loss = {epoch_train_loss:.4f}, Val Loss = {val_loss:.4f}\")\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        best_model_state = model.state_dict()\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= early_stopping_patience:\n",
    "            print(f\"Stopping early at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "model.load_state_dict(best_model_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d74425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the learning curve with dropout\n",
    "plt.plot(train_losses, label=\"train\")\n",
    "plt.plot(val_losses, label=\"val\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e90aea01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE on train data (dropout):  0.46103065079775535\n",
      "RMSE on test data (dropout):  0.45557599217372446\n"
     ]
    }
   ],
   "source": [
    "# Evaluate with dropout model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predict_train = model(x_train_tensor).numpy()\n",
    "    predict_test = model(x_test_tensor).numpy()\n",
    "\n",
    "print(\"RMSE on train data (dropout): \", mean_squared_error(train_y, predict_train) ** 0.5)\n",
    "print(\"RMSE on test data (dropout): \", mean_squared_error(test_y, predict_test) ** 0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d074d59d",
   "metadata": {},
   "source": [
    "# 4. Model building with images\n",
    "\n",
    "Now lets first try a simple regression simply with the images of the flats. Spoiler: The results are really bad (worse than mean prediction). So this is really just a demonstration of how such models roughly look. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "88f93377",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0883d898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split image data\n",
    "img_train_x, img_test_x, img_train_y, img_test_y = train_test_split(\n",
    "    train_images_flat, Y, test_size=0.25, random_state=0\n",
    ")\n",
    "\n",
    "# Split image data\n",
    "img_train_x, img_test_x, img_train_y, img_test_y = train_test_split(\n",
    "    train_images_flat, Y, test_size=0.25, random_state=0\n",
    ")\n",
    "\n",
    "# Fix: convert object-type arrays to uniform float32 arrays\n",
    "img_train_x = np.stack(img_train_x).astype(np.float32)\n",
    "img_test_x = np.stack(img_test_x).astype(np.float32)\n",
    "\n",
    "# Convert image arrays from NHWC to NCHW for PyTorch\n",
    "img_train_x = torch.tensor(img_train_x).permute(0, 3, 1, 2)\n",
    "img_test_x = torch.tensor(img_test_x).permute(0, 3, 1, 2)\n",
    "\n",
    "# Convert labels to torch tensors\n",
    "img_train_y = torch.tensor(img_train_y.to_numpy(), dtype=torch.float32).view(-1, 1)\n",
    "img_test_y = torch.tensor(img_test_y.to_numpy(), dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_ds = TensorDataset(img_train_x, img_train_y)\n",
    "test_ds = TensorDataset(img_test_x, img_test_y)\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "test_dl = DataLoader(test_ds, batch_size=batch_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "92122399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN Model\n",
    "class CNNRegressor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "\n",
    "            nn.Conv2d(128, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(64 * 2 * 2, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "cnn_model = CNNRegressor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afe3bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train CNN\n",
    "def train_model(model, train_dl, val_dl=None, epochs=100):\n",
    "    opt = optim.Adam(model.parameters(), lr=0.001)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    train_loss, val_loss = [], []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for xb, yb in train_dl:\n",
    "            preds = model(xb)\n",
    "            loss = loss_fn(preds, yb)\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            epoch_loss += loss.item() * xb.size(0)\n",
    "\n",
    "        train_loss.append(epoch_loss / len(train_dl.dataset))\n",
    "\n",
    "        if val_dl:\n",
    "            model.eval()\n",
    "            val_epoch_loss = 0\n",
    "            with torch.no_grad():\n",
    "                for xb, yb in val_dl:\n",
    "                    val_epoch_loss += loss_fn(model(xb), yb).item() * xb.size(0)\n",
    "            val_loss.append(val_epoch_loss / len(val_dl.dataset))\n",
    "\n",
    "        print(f\"Epoch {epoch+1}: train loss = {train_loss[-1]:.4f}\")\n",
    "\n",
    "    return train_loss, val_loss\n",
    "\n",
    "cnn_model = CNNRegressor()\n",
    "train_losses, _ = train_model(cnn_model, train_dl, epochs=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3895de0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate CNN\n",
    "cnn_model.eval()\n",
    "with torch.no_grad():\n",
    "    pred_train = cnn_model(img_train_x).numpy()\n",
    "    pred_test = cnn_model(img_test_x).numpy()\n",
    "\n",
    "print(\"RMSE train:\", mean_squared_error(img_train_y, pred_train) ** 0.5)\n",
    "print(\"RMSE test:\", mean_squared_error(img_test_y, pred_test) ** 0.5)\n",
    "\n",
    "plt.plot(train_losses, label=\"train\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17503fed",
   "metadata": {},
   "source": [
    "As this is just an introductory notebook, I again want to emphasize that the performance of these models is arbitrary and should not effect your model or variable choice. I did not try to get these models to work. I just wanted to showcase you some stuff.\n",
    "\n",
    "In this notebook we have started with very simple methods, suited to analyse structured data. Even in the performed models any form of optimizing is missing and the variable selection was done at random. Therefore you have plenty of room to build better fitting models using more suitable variables.\n",
    "\n",
    "Subsequently we have used 2 types of neural networks:\n",
    "* Multi Layer Perceptrons to analyze structured data\n",
    "* Convolutional Neural networks to analyze images\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
